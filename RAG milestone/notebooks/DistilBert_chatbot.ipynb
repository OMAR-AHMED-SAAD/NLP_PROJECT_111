{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"validation\")  \n",
    "\n",
    "contexts = dataset[\"context\"]\n",
    "questions = dataset[\"question\"]\n",
    "answers = dataset[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document \n",
    "import json\n",
    "\n",
    "contexts_with_qa: Dict[str, List[Dict]] = {}\n",
    "for i, ex in tqdm(enumerate(dataset), total=len(ds)):\n",
    "        ctx = ex[\"context\"]\n",
    "        ans_txt = ex[\"answers\"][\"text\"][0] if ex[\"answers\"][\"text\"] else \"\"\n",
    "        ans_st = ex[\"answers\"][\"answer_start\"][0] if ex[\"answers\"][\"answer_start\"] else \"\"\n",
    "        qid = ex.get(\"id\", str(i))\n",
    "        qa_pair = {\n",
    "        \"question\":ex.get(\"question\", \"\"),\n",
    "        \"answer_text\": ans_txt,\n",
    "        \"answer_start\": ans_st,\n",
    "        \"id\": qid\n",
    "        }\n",
    "\n",
    "        contexts_with_qa.setdefault(ctx, []).append(qa_pair)\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "for ctx, qa_list in contexts_with_qa.items():\n",
    "        texts.append(ctx)\n",
    "        metadatas.append({\"qas\": json.dumps(qa_list)})\n",
    "\n",
    "docs = [Document(page_content=text, metadata=meta)\n",
    "        for text, meta in zip(texts, metadatas)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embedding_model)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. Initialize memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 2. (Re)define retriever and qa_pipeline if neededâ€”\n",
    "#    otherwise skip these two lines\n",
    "# from transformers import pipeline\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# 3. Your original retrieval + QA code, now with memory\n",
    "query = \"Tell me about Jay Z and Beyonce\"\n",
    "\n",
    "# Step 3a. Retrieve relevant docs\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "\n",
    "# Step 3b. Build docs context\n",
    "k = 2  # Number of relevant docs to retrieve\n",
    "docs_context = \"\\n\\n\".join([d.page_content for d in docs[:k]])\n",
    "\n",
    "# Step 3c. Pull history from memory\n",
    "history_msgs = memory.chat_memory.messages  # list of Message objects\n",
    "history_str = \"\\n\".join(\n",
    "    f\"{msg.type.capitalize()}: {msg.content}\" for msg in history_msgs\n",
    ")\n",
    "\n",
    "# Step 3d. Combine history + docs\n",
    "full_context = (history_str + \"\\n\\n\" + docs_context) if history_str else docs_context\n",
    "\n",
    "print(full_context)\n",
    "\n",
    "# Step 4. Call the QA pipeline\n",
    "result = qa_pipeline(question=query, context=full_context)\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "\n",
    "# Step 5. Store this turn in memory\n",
    "memory.chat_memory.add_user_message(query)\n",
    "memory.chat_memory.add_ai_message(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_kernel",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
